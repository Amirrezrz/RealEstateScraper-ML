{
 "cells": [
  {
   "cell_type": "code",
   "id": "573b567a-babb-4cf4-9b16-bcee63b8ad25",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#data from user\n",
    "city = input(\"The city that you want to exctract data(in Italian): \").strip()\n",
    "num_pages = int(input(\"how many pages would you want to exctract: \"))\n",
    "\n",
    "# browser setting\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)  #it keeps browser open to skip the captcha\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# run driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# save all the data in a file\n",
    "with open(\"all_pages.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for page in range(1, num_pages + 1):\n",
    "        if page == 1:\n",
    "            url = f\"https://www.immobiliare.it/vendita-case/{city}/\"\n",
    "        else:\n",
    "            url = f\"https://www.immobiliare.it/vendita-case/{city}/?pag={page}\"\n",
    "        \n",
    "        print(f\"reciving data: {url}\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        input(\"please skip captcha then press enter\")  #waiting for captcha\n",
    "\n",
    "        file.write(f\"<!-- Page {page} -->\\n\")\n",
    "        file.write(driver.page_source)\n",
    "        file.write(\"\\n\\n\")\n",
    "    \n",
    "print(\"✅ all the pages saved in a file\")\n",
    "driver.quit()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "124164d7-90a5-4ac9-8a61-12065b06fe05",
   "metadata": {},
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read html file\n",
    "with open(\"all_pages.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# extract prices\n",
    "prices = [price.get_text(strip=True) for price in soup.find_all(\"div\", class_=\"in-listingCardPrice\")]\n",
    "\n",
    "#extract size of the houses\n",
    "sizes = [size.get_text(strip=True) for size in soup.find_all(\"div\", class_=\"in-listingCardFeatureList__item\") if \"m²\" in size.get_text()]\n",
    "\n",
    "# number of rooms\n",
    "rooms = [room.get_text(strip=True) for room in soup.find_all(\"div\", class_=\"in-listingCardFeatureList__item\") if \"locali\" in room.get_text()]\n",
    "\n",
    "# save to csv file\n",
    "# it would be used to train my ML model\n",
    "with open(\"data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Price (€)\", \"Size (m²)\", \"Rooms\"])  #write headers\n",
    "    for price, size, room in zip(prices, sizes, rooms):\n",
    "        writer.writerow([price, size, room])\n",
    "\n",
    "print(\"✅ Data successfully saved in 'data.csv'!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8e564708ab19ce4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
